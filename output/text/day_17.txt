Title: Day 17: AI as SRE: Is This Code Operable?
Day: 17
Date: Jan 17, 2026
Characters: 4902
Words: 769
==================================================

Welcome to Day 17 of 31 Days of Vibe Coding. Today's topic is: AI as SRE: Is This Code Operable?.

Imagine this. Your feature works perfectly in development. You ship it. A week later, something’s wrong. Users mention weird behavior. You check the logs.

There are no logs.

You check the metrics. There are no metrics. You check for errors. Nothing. The code is a black box. It’s doing something, but you have no idea what.

This is the difference between code that works and code you can operate. AI is excellent at generating the first kind. It has no idea the second kind matters unless you tell it.

Working code and operable code are different things. AI needs to be told to care about both.

What Makes Code Operable

Operable code answers these questions:

Is it working? Can I tell at a glance if this feature is healthy?

What’s happening? Can I see what the code is doing right now?

What went wrong? When it fails, can I find out why?

How do I fix it? Do I have the tools to diagnose and resolve issues?

AI-generated code often works but fails all four questions. It’s optimized for functionality, not operability.

The SRE Audit Prompt

Here’s the prompt that catches operability gaps:

Showing a prompt template that instructs an AI to review code from an SRE perspective, checking for observability, failure modes, debuggability, recoverability, and alerting gaps.

A Real SRE Audit

Here’s code AI generated for processing card trades:

Showing a TypeScript function that processes an accepted trade by swapping card ownership between two users and marking the trade as completed.

The SRE audit found:

Observability Gaps:

No logging at all. If this fails, you won’t know it ran.

No metrics. You can’t track trade processing rate or success rate.

Silent early return. If trade is missing or wrong status, nothing is logged.

Failure Mode Issues:

Partial failure possible. If it fails after transferring some cards, trade is in inconsistent state.

No error handling. Database errors will crash silently.

No indication of what failed or why.

Debuggability Issues:

Can’t trace which trades processed or failed.

Can’t tell how long processing took.

Can’t identify which card transfer failed in a multi-card trade.

Recoverability Issues:

No way to retry failed trades.

No idempotency. Running twice could cause issues.

No transaction. Partial state is possible.

Fixed version:

Showing a TypeScript function that processes a card trade between users within a database transaction, with logging and metrics tracking throughout.

The fixed version tells you everything: what’s processing, what succeeded, what failed, how long it took, and why.

The Logging Audit Prompt

For focused logging review:

Showing a code review prompt template that guides reviewers through checking logging completeness, context, and security for each operation.

The Metrics Audit Prompt

For metrics coverage:

Showing a prompt template that asks an AI to analyze code and recommend observability metrics across throughput, latency, errors, saturation, and business categories.

The Failure Mode Prompt

For comprehensive failure analysis:

Showing a prompt template that asks an AI to analyze code failure modes across categories like network issues, database failures, and concurrency problems, with structured questions about causes, detection, and recovery.

The Runbook Generation Prompt

For operational documentation:

Showing a prompt template that asks an AI to generate an operational runbook for a software feature, including sections for health checks, troubleshooting, and recovery procedures.

The 3am Test

For every piece of code, ask:

Showing a prompt template with five diagnostic questions for evaluating code debuggability during on-call incidents.

If the answer to any question is “I don’t know,” the code isn’t ready for production.

The Operability Checklist

Before shipping:

Showing a production readiness checklist covering logging, metrics, alerting, failure handling, and operational documentation requirements.

Building Operability Into Prompts

Don’t audit after. Build it in from the start:

Showing a prompt template that instructs an AI to build production-ready features with comprehensive logging, metrics, error handling, and health checks.

Tomorrow

You’ve audited for operability. Now who writes the tests? Tomorrow I’ll show you how to use AI as a test generator, creating comprehensive test suites that catch bugs before they reach the code you just made operable.

Try This Today

Take a piece of AI-generated code that’s in production

Run the SRE audit prompt

Ask: “If this broke at 3am, what would I wish I had?”

The gap between what you have and what you’d wish for is your operability debt. Start paying it down before you get that 3am page.

That's it for Day 17. Join us tomorrow for Day 18. Thanks for listening to 31 Days of Vibe Coding.